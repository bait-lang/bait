// Copyright (c) 2023-present Lukas Neubert.
// This Source Code is subject to the terms of the Mozilla Public License 2.0.
package tokenizer

import bait.token
import bait.errors

struct Tokenizer {
	path string
	text string
mut:
	pos i32
	line i32
	last_nl_pos i32
	is_string_inter bool
	str_quote u8
	should_abort bool
pub mut:
	errors []errors.Message
}

// TODO remove once these things are implemented
//   - default struct values
//   - field required attribute
pub fun new_tokenizer(text string, path string) Tokenizer {
	return Tokenizer{
		path = path
		text = text
		pos = -1
		line = 1
		last_nl_pos = -1
	}
}

pub fun (mut t Tokenizer) tokenize() []token.Token {
	mut tokens := []token.Token
	for not t.should_abort {
		tokens.push(t.next_token())
	}
	return tokens
}

fun (t Tokenizer) new_token(kind token.TokenKind, val string) token.Token {
	return token.Token{
		kind = kind
		val = val
		pos = token.Pos{
			line = t.line
			col = t.pos - t.last_nl_pos
		}
	}
}

fun (mut t Tokenizer) eof_token() token.Token {
	t.should_abort = true
	return t.new_token(.eof, "")
}

fun (mut t Tokenizer) next_token() token.Token {
	for true {
		t.pos += 1
		t.skip_whitespace()
		if t.pos >= t.text.length {
			return t.eof_token()
		}

		c := t.text[t.pos]
		if is_name_start_char(c) {
			return t.name_or_keyword()
		} else if is_digit(c) {
			num := t.number_val()
			return t.new_token(.number, num)
		}
		nextc := t.text[t.pos + 1]
		match c {
			`'`, `\"` {
				str := t.string_val(c)
				return t.new_token(.string, str)

			}
			`\`` {
				val := t.char_val()
				return t.new_token(.char, val)
			}
			`.` {
				return t.new_token(.dot, "")
			}
			`,` {
				return t.new_token(.comma, "")
			}
			`+` {
				if nextc == `=`{
					t.pos += 1
					return t.new_token(.plus_assign, "")
				}
				return t.new_token(.plus, "")
			}
			`-` {
				if nextc == `=`{
					t.pos += 1
					return t.new_token(.minus_assign, "")
				}
				return t.new_token(.minus, "")
			}
			`*` {
				if nextc == `=`{
					t.pos += 1
					return t.new_token(.mul_assign, "")
				}
				return t.new_token(.mul, "")
			}
			`/` {
				if nextc == `=`{
					t.pos += 1
					return t.new_token(.div_assign, "")
				}
				if nextc == `/`{
					t.ignore_line()
					continue
				}
				return t.new_token(.div, "")
			}
			`%` {
				if nextc == `=`{
					t.pos += 1
					return t.new_token(.mod_assign, "")
				}
				return t.new_token(.mod, "")
			}
			`=` {
				if nextc == `=`{
					t.pos += 1
					return t.new_token(.eq, "")
				}
				return t.new_token(.assign, "")
			}
			`:` {
				if nextc == `=`{
					t.pos += 1
					return t.new_token(.decl_assign, "")
				}
				return t.new_token(.colon, "")
			}
			`;` {
				return t.new_token(.semicolon, "")
			}
			`!` {
				if nextc == `=`{
					t.pos += 1
					return t.new_token(.ne, "")
				}
			}
			`<` {
				if nextc == `=`{
					t.pos += 1
					return t.new_token(.le, "")
				}
				return t.new_token(.lt, "")
			}
			`>` {
				if nextc == `=`{
					t.pos += 1
					return t.new_token(.ge, "")
				}
				return t.new_token(.gt, "")
			}
			`(` {
				return t.new_token(.lpar, "")
			}
			`)` {
				return t.new_token(.rpar, "")
			}
			`[` {
				return t.new_token(.lbr, "")
			}
			`]` {
				return t.new_token(.rbr, "")
			}
			`{` {
				return t.new_token(.lcur, "")
			}
			`}` {
				if t.is_string_inter {
					t.is_string_inter = false
					str := t.string_val(t.str_quote)
					return t.new_token(.string, str)
				}
				return t.new_token(.rcur, "")
			}
			`&` {
				return t.new_token(.amp, "")
			}
			`^` {
				return t.new_token(.caret, "")
			}
			`|` {
				return t.new_token(.pipe, "")
			}
			`$` {
				return t.new_token(.dollar, "")
			}
			`#` {
				t.pos += 1
				name := t.name_val()
				return t.new_token(.hash, name)
			}
			`@` {
				t.pos += 1
				name := t.name_val()
				return t.new_token(.attr, name)
			}
			else {}
		}
		t.error('unknown char `${c.ascii()}`')
	}

	return t.eof_token()
}

fun (mut t Tokenizer) name_or_keyword() token.Token {
	name := t.name_val()
	kind := token.keyword_to_kind(name)
	if kind != .name {
		return t.new_token(kind, '')
	}
	return t.new_token(.name, name)
}

fun (mut t Tokenizer) name_val() string{
	start := t.pos
	t.pos += 1
	for t.pos < t.text.length {
		if not is_name_char(t.text[t.pos]) {
			break
		}
		t.pos += 1
	}
	t.pos -= 1
	return t.text.substr(start, t.pos + 1)
}

fun (mut t Tokenizer) number_val() string {
	start := t.pos
	for is_digit(t.text[t.pos]) {
		t.pos += 1
	}
	t.pos -= 1
	return t.text.substr(start, t.pos + 1)
}

fun (mut t Tokenizer) string_val(quote u8) string {
	is_js := t.text[t.pos - 1] == `.`
	start_line := t.line
	start := t.pos + 1
	for not t.should_abort {
		t.pos += 1
		if t.pos >= t.text.length {
			t.error_with_line('unfinished string literal', start_line)
		}
		c := t.text[t.pos]
		if c == `\n` {
			t.last_nl_pos = t.pos
			t.line += 1
		} else if c == `\\` {
			t.pos += 1
		} else if not is_js and c == `$` and t.text[t.pos + 1] == `{` {
			t.is_string_inter = true
			t.str_quote = quote
			t.pos -= 1
			return t.text.substr(start, t.pos + 1)
		} else if c == quote {
			break
		}
	}
	return t.text.substr(start, t.pos)
}

fun (mut t Tokenizer) char_val() string {
	start := t.pos + 1
	for true {
		t.pos += 1
		c := t.text[t.pos]
		if c == `\\` {
			t.pos += 1
		} else if c == `\`` {
			break
		}
	}
	return t.text.substr(start, t.pos)
}

fun (mut t Tokenizer) skip_whitespace(){
	for t.pos < t.text.length {
		c := t.text[t.pos]
		if c == `\n` {
			t.last_nl_pos = t.pos
			t.line += 1
		} else if not [` `, `\t`, `\r`].contains(c) {
			return
		}
		t.pos += 1
	}
}

fun (mut t Tokenizer) ignore_line() {
	for t.pos < t.text.length and t.text[t.pos] != `\n` {
		t.pos += 1
	}
	t.last_nl_pos = t.pos
	t.line += 1
}

fun (mut t Tokenizer) error(msg string){
	t.error_with_line(msg, t.line)
}

fun (mut t Tokenizer) error_with_line(msg string, line i32){
	t.errors.push(errors.Message{
		kind = .error
		path = t.path
		pos = token.Pos{
			line = line
			col = t.pos - t.last_nl_pos
		}
		title = 'error'
		msg = msg
	})

	t.should_abort = true
}

fun is_name_start_char(c u8) bool {
	return c >= `a` and c <= `z` or c >= `A` and c <= `Z` or c == `_`
}

fun is_name_char(c u8) bool {
	return is_name_start_char(c) or is_digit(c)
}

fun is_digit(c u8) bool {
	return c >= `0` and c <= `9`
}
